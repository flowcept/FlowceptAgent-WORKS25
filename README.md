# FlowceptAgent-WORKS25
This repo enables evaluation of LLM agents for workflow provenance query answering, using Flowcept for workflow execution tracing, and GPT-4 as a judge for scoring LLM-generated code.

**'_LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology__'**
# To get everything set up flowcept wise and model running.
- Make sure you have Docker downloaded and Mongodb
- Download flowcept with all dependencies use:
```
  bash pip install flowcept[llm_agent]
```

- You can also do this
```
git clone https://github.com/ORNL/flowcept.git
cd flowcept
pip install .[llm_agent]
```

- For more instructions on how to install flowcept: 'https://github.com/ORNL/flowcept/tree/main'
- Once flowcept is installed, make sure you have conda installed as that is what you will be using for your environment.
- To run the agent make sure you have a working API key and llm server url as you will input them into the 'settings.yaml' file under 'agent' to get the agent to run.
- To use Chat GPT make sure you have azure installed.
- Once flowcept is downloaded and ensure your agent configuration is correctly specified in settings.yaml: Use this code to start up your docker server in terminal.
```
docker compose --file deployment/compose-mongo.yml up --detach
```

# How to start Flowcept Agent
1. To start flowcept you must first be in your conda virtual environment with docker running.
2. To get in the virtual environment type in conda activate name_of_environment
3. Type in
```
flowcept --start-agent.
```
It should start as long as flowcept is properly installed and docker is running.
## OPTIONAL
To start the Flowcept GUI to run queries use this command in terminal after the agent is running. - Note it will not produce the csvs that are used in the plots it will only respond to the queries.
```
flowcept --start-agent-gui
```
## BEFORE EVERY RUN MAKE SURE: 
- At the top of scoring.py there is a setup for parameters that is essential for how the csv and plots work.
- Look for MODEL, CONFIG, RAG, AND WFE. The scoring.py file tells you what these are, but they need to be changed per model run if you are running different combinations 			of Model, Context, and workflow run types
- For instance, If you were running GPT-4 with Baseline Context on 100 workflows ( you can ignore config as its conventions are for custom temp and top_p configurations for testing ) Then Model = GPT-4, RAG = BL, WFE - 100. Again this is at the top of 'scoring.py', and is only so that the csv reads in that configuration. So when the plots are ran they split correctly into their substrings of Model, RAG, etc.

# Running a Workflow + Scoring
It must be noted that scoring.py has two functions
- It runs the queries and calls for a LLM to respond to them by generating pandas df code on the 'current_agent_df' that is generated from running the workflow.
- It also calls for a second LLM that acts as the judge to score the query responses generated by the first llm based on strict guidelines and rules.
- 'Scoring.py' contains the query set coupled with the golden truth query responses that the Judge scores the generated with.

**1.** This should be open in another terminal. Any change you make before calling 'scoring.py' you must save for it to show up in the generated csv.

**2.** Make sure you have an .env file that contains these things( we have to source to it later after we run the agent in a seperate terminal from the agent running for the LLM judge to work ):
		export SAMBASTUDIO_URL= x
		export SAMBASTUDIO_API_KEY= x
		export OPENAI_API_KEY= x
		export AZURE_OPENAI_API_ENDPOINT= x
		export AZURE_OPENAI_API_KEY= x 
	
**3.** After your agent is running you need to open another terminal seperate from the agent's that you will run in from here on out when running experiments ( Split terminal if in vscode or other app to see interaction )

**4.** In that terminal make sure you are in the same environment and path as the agent.

**5.** OPTIONAL, FIXED AN ISSUE WITH WSL AND THE URL INSIDE THE .env WITH SCORING : 'dos2unix .env path' Running this should fix any formatting issues if you are using wsl to run flowcept.

**6.** Source to your .env file. Only needs to be done once per agent session (not per workflow run).

**7.** Next, run your 'workflow.py' file. One will be attached in this repository. Just run it with python3 path of the file in the same terminal.

**8.** Running the workflow should save a csv that has the schema and 'current_agent_df' which is what the model when you run 'scoring.py will use to answer queries. 
**9.** Finally, after the workflow runs, run 'scoring.py' the same way as the workflow.

**10.** 'Scoring.py' will generate a csv that contains the query responses by the chosen LLM, metrics such as 'time_taken_seconds', L_Score ( LLM Judge Score ), L_Response ( LLM Judge Reasoning for the score it gave ). This CSV will be combined with others to generate data in the jupyter notebook. 

# How to interact with 'in_memory_query_prompts'
1. For experimentation, the only section that was tinkered with is the 'generate_plot_code_prompt' code.
2. This code allows for easy switching between what the context is for the model llm.
3. All you need to do to test with this is: Make sure your model is correct with api key and url. 
4. **BEFORE YOU START YOUR AGENT** when you make a change inside 'in_memory_query_prompts' you must always save it before you start the agent or it will not apply. 
5. If you want to test a model without the context from query guidelines you would simply comment out '#' that line and save it then start the agent and run as told above.
6. For instance to run a 'baseline' context on a model you would comment out the schema prompt, guidelines, and fewshot lines.
7. To not include example values in your context combination you have to scroll up to the tagged out example values and uncomment it.
8. That is all you need to change to run the same experiments. Labels for the context combinations are mentioned in the plotting code with maps and the paper.

# Plotting Results
1. Once you download the notebook, make sure you know the paths of all generated csvs from running 'scoring.py'.
2. Put them into a folder, input the path of the folder into the code at the top where it prompts you. 
3. As long as your csvs follow the example 'combined.csv' that will be implemented then the plots will run. Note- You might need to make sure your labels match what the plots call. 
# Notes on GPT Judge
- Chat GPT-4 is used as the judge.
- It uses the 'EVAL_PROMPT' inside of 'scoring.py' to score the generated code from the model that responds to the queries and gives a pandas df code.
- The judge uses the prompts in 'EVAL_PROMPT' along with a golden pandas dataframe code response that was created by a human to Judge the other LLM's response.
- The 'EVAL_PROMPT' section describes how scoring is to be understood by the GPT Judge
# Documentation
The full flowcept documentation is available at [Documentation](https://flowcept.readthedocs.io/). 
# Acknowledgement
#### This research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725.
#### This research was supported in part by an appointment to the Education Collaboration at Oak Ridge National Laboratory Program (ECO), sponsored by the U.S. Department of Energy and administered by the Oak Ridge Institute for Science and Education.
